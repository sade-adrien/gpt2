[
    {
        "step": 0,
        "tokens": 0,
        "learning_rate": 0.00011999999999999999,
        "gradient_norm": 2.142289400100708,
        "train_loss": 10.835622787475586,
        "val_loss": 10.831964492797852,
        "hellaswag_acc": 0.25871342420578003,
        "hellaswag_acc_norm": 0.24218282103538513
    },
    {
        "step": 1,
        "tokens": 64,
        "learning_rate": 0.00023999999999999998,
        "gradient_norm": 2.13016414642334,
        "train_loss": 10.832956314086914,
        "val_loss": null,
        "hellaswag_acc": null,
        "hellaswag_acc_norm": null
    },
    {
        "step": 2,
        "tokens": 128,
        "learning_rate": 0.00035999999999999997,
        "gradient_norm": 1.9255295991897583,
        "train_loss": 10.828201293945312,
        "val_loss": null,
        "hellaswag_acc": null,
        "hellaswag_acc_norm": null
    },
    {
        "step": 3,
        "tokens": 192,
        "learning_rate": 0.00047999999999999996,
        "gradient_norm": 2.0148332118988037,
        "train_loss": 10.808344841003418,
        "val_loss": null,
        "hellaswag_acc": null,
        "hellaswag_acc_norm": null
    },
    {
        "step": 4,
        "tokens": 256,
        "learning_rate": 0.0006,
        "gradient_norm": 1.7167061567306519,
        "train_loss": 10.829193115234375,
        "val_loss": null,
        "hellaswag_acc": null,
        "hellaswag_acc_norm": null
    },
    {
        "step": 5,
        "tokens": 320,
        "learning_rate": 0.0005999999999999998,
        "gradient_norm": 1.4834668636322021,
        "train_loss": 10.808290481567383,
        "val_loss": 10.819784164428711,
        "hellaswag_acc": 0.2589125633239746,
        "hellaswag_acc_norm": 0.24825730919837952
    },
    {
        "step": 6,
        "tokens": 384,
        "learning_rate": 0.0005940998521981274,
        "gradient_norm": 1.5911372900009155,
        "train_loss": 10.773300170898438,
        "val_loss": null,
        "hellaswag_acc": null,
        "hellaswag_acc_norm": null
    },
    {
        "step": 7,
        "tokens": 448,
        "learning_rate": 0.0005766572735635021,
        "gradient_norm": 1.2533371448516846,
        "train_loss": 10.80211067199707,
        "val_loss": null,
        "hellaswag_acc": null,
        "hellaswag_acc_norm": null
    },
    {
        "step": 8,
        "tokens": 512,
        "learning_rate": 0.0005484345884812357,
        "gradient_norm": 1.509485125541687,
        "train_loss": 10.819239616394043,
        "val_loss": null,
        "hellaswag_acc": null,
        "hellaswag_acc_norm": null
    },
    {
        "step": 9,
        "tokens": 576,
        "learning_rate": 0.0005106652637168916,
        "gradient_norm": 1.4093266725540161,
        "train_loss": 10.773098945617676,
        "val_loss": null,
        "hellaswag_acc": null,
        "hellaswag_acc_norm": null
    },
    {
        "step": 10,
        "tokens": 640,
        "learning_rate": 0.0004649999999999999,
        "gradient_norm": 1.2755498886108398,
        "train_loss": 10.76418685913086,
        "val_loss": 10.790103912353516,
        "hellaswag_acc": 0.2597092390060425,
        "hellaswag_acc_norm": 0.2544313669204712
    },
    {
        "step": 11,
        "tokens": 704,
        "learning_rate": 0.00041343458848123576,
        "gradient_norm": 1.0741602182388306,
        "train_loss": 10.79460334777832,
        "val_loss": null,
        "hellaswag_acc": null,
        "hellaswag_acc_norm": null
    },
    {
        "step": 12,
        "tokens": 768,
        "learning_rate": 0.0003582226850822664,
        "gradient_norm": 1.1638264656066895,
        "train_loss": 10.742413520812988,
        "val_loss": null,
        "hellaswag_acc": null,
        "hellaswag_acc_norm": null
    },
    {
        "step": 13,
        "tokens": 832,
        "learning_rate": 0.0003017773149177335,
        "gradient_norm": 0.9698898792266846,
        "train_loss": 10.730567932128906,
        "val_loss": null,
        "hellaswag_acc": null,
        "hellaswag_acc_norm": null
    },
    {
        "step": 14,
        "tokens": 896,
        "learning_rate": 0.0002465654115187642,
        "gradient_norm": 1.1147528886795044,
        "train_loss": 10.740471839904785,
        "val_loss": null,
        "hellaswag_acc": null,
        "hellaswag_acc_norm": null
    },
    {
        "step": 15,
        "tokens": 960,
        "learning_rate": 0.00019500000000000002,
        "gradient_norm": 1.1493314504623413,
        "train_loss": 10.759958267211914,
        "val_loss": 10.76216983795166,
        "hellaswag_acc": 0.25980880856513977,
        "hellaswag_acc_norm": 0.2515435218811035
    },
    {
        "step": 16,
        "tokens": 1024,
        "learning_rate": 0.00014933473628310834,
        "gradient_norm": 1.0554919242858887,
        "train_loss": 10.709583282470703,
        "val_loss": null,
        "hellaswag_acc": null,
        "hellaswag_acc_norm": null
    },
    {
        "step": 17,
        "tokens": 1088,
        "learning_rate": 0.00011156541151876421,
        "gradient_norm": 1.0614491701126099,
        "train_loss": 10.696703910827637,
        "val_loss": null,
        "hellaswag_acc": null,
        "hellaswag_acc_norm": null
    },
    {
        "step": 18,
        "tokens": 1152,
        "learning_rate": 8.334272643649772e-05,
        "gradient_norm": 1.015507459640503,
        "train_loss": 10.728385925292969,
        "val_loss": null,
        "hellaswag_acc": null,
        "hellaswag_acc_norm": null
    },
    {
        "step": 19,
        "tokens": 1216,
        "learning_rate": 6.590014780187246e-05,
        "gradient_norm": 1.3925564289093018,
        "train_loss": 10.79597282409668,
        "val_loss": 10.752580642700195,
        "hellaswag_acc": 0.2602071166038513,
        "hellaswag_acc_norm": 0.25074684619903564
    }
]